# python setup.py sdist bdist_wheel
# pip install dist\halfspaceod_pkg-0.1.0-py3-none-any.whl --force-reinstall
# spark-submit --driver-class-path %SPARK_HOME%\postgresql-42.2.18.jar --driver-memory 6g --executor-memory 6g  example.py 

from halfspaceod.common.install_libraries import install_python_libraries

# Install all the libraries needed via pip.
install_python_libraries()

from halfspaceod.etl.etl_pipeline_script import * 
from halfspaceod.common.common import *
from halfspaceod.ip_enrichment.geo_ip import *
from halfspaceod.ip_enrichment.whois import *
from halfspaceod.detection.model_pipeline import * 
from halfspaceod.etl.postgres import * 
from halfspaceod.detection.inference import * 

print("Calling Start Spark!")
spark, db_name = start_spark()
print("Started spark!")
print("")

# Create the GeopIP databases of Geolocations.
setup_geoip_database("PATH TO GEOLITE FOLDER")

# Create empty IPtoGeo and IPtoCIDR Databases, for future merges.
setup_clean_ip_to_geo_and_cidr_tables()

# Run first ETL, here for January month.
suffix = "January"
day_start = "2021-01-01"
day_end   = "2021-02-01"
add_extra_day = True 
overwrite_data = False 

# Call the pipeline.
run_etl(suffix, day_start, day_end, add_extra_day=True, overwrite_data=False)

# Issue:
# First run of ETL has no whois information. 
# After the first ETL run, they are loaded into our system and we can call whois. 
# run_whois() looks at all IPs in IPtoGeo which were written in run_etl, so it takes no inputs.
# It attempts to lookup every IP we have no CIDR information on yet.
# (Takes a _long_ time)
run_whois()

# Call pipeline again, to use the CIDR information again.
# Since overwrite_data_data is false, we don't read postgres again, and use the previous call's tables.
run_etl(suffix, day_start, day_end, add_extra_day=True, overwrite_data=False)



# Model pipeline information 
suffix = suffix
run_name = "JanuaryModels"
service = "-"
split_date = "2020-10-25"
should_use_split_date = False
contamination = 0.0045

should_log_shap = True

category_list = ["SensorID", "Protocol", "ConnectionState", "SenderContinent", "ResponderContinent", "SenderIPDescription", "ResponderIPDescription"]

sensor_list = [13]
should_log_to_mlflow = True
test_data_in_specific_sensor = False
test_sensor = 0

best_hbos_info = {"contamination" : contamination, "n_bins" : 0}
best_if_info   = {"contamination" : contamination, "n_estimators" : 100, "max_samples" : 0.05}

model_A_name = "HBOS"
model_B_name = "IF"
model_A_info = BestHBOSInfo
model_B_info = BestIFInfo

# Train a run with two models (Isolation Forest and Histogram-Based Outlier Detection) under name "JanuaryModels"
res = model_pipeline(run_name, suffix, service, sensorList, split_date, should_use_split_date, 
                     contamination, should_log_shap, model_A_name, model_A_info, model_B_name, model_B_info, 
                     category_list, should_log_to_mlflow, test_data_in_specific_sensor, test_sensor)



# Run first ETL, here for January month.
suffix = run_name + "Inference"
day_start = "2021-02-01"
day_end   = "2021-02-08"
add_extra_day = True 
overwrite_data = False 

model_name = model_A_name    
    
# Run inference with ETL, using a new day-range to specify inference target.
# Loads the models and info from the previous run. 
# Requires the Model Name from the previous run ("HBOS", "IF", "LODA", etc.), list of sensors to infer on, and service ("-" = conn, "http" = http and so on.)   
preds = inference(run_name, model_name, sensor_list, service, day_start, day_end)

# If the data is already available in a suffix, you can also infer directly on that.
preds = inference_from_suffix(run_name, model_name, sensor_list, service, suffix)

# Print the resulting dataframe.
spark.createDataFrame(preds).show(truncate=False)

print("")
print("Finished!")
print("")